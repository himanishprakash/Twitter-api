{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the libraries which are need for the project\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "style.use('ggplot')\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "from matplotlib import colors as mcolors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "from apify_client import ApifyClient\n",
    "\n",
    "import base64\n",
    "from email.mime.text import MIMEText\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from requests import HTTPError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL (Extracting, transformation and loading of twitter informs data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "api_token = 'Apify Api token'\n",
    "actor_id = \"heLL6fUofdPgRXZie\"\n",
    "searchterm = \"#informs2023\"\n",
    "\n",
    "def extract_tweets(api_token, actor_id, searchhashtag):\n",
    "    '''\n",
    "    Extract tweets using Apify API.\n",
    "    \n",
    "    Parameters:\n",
    "    api_token (string): Personal API token obtained from Apify.\n",
    "    actor_id (string): Scraper ID provided by Apify.\n",
    "    searchhashtag (string): Hashtag to search for tweets.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    '''\n",
    "    # Initialize Apify client\n",
    "    client = ApifyClient(api_token)\n",
    "\n",
    "    # Define input for the Apify actor run\n",
    "    run_input = {\n",
    "        \"searchTerms\": [searchhashtag],\n",
    "        \"searchMode\": \"live\",\n",
    "        \"addUserInfo\": True,\n",
    "        \"scrapeTweetReplies\": True,\n",
    "        \"urls\": [\"https://twitter.com/search?q=gpt&src=typed_query&f=live\"],\n",
    "    }\n",
    "\n",
    "    # Run the actor and get the dataset ID\n",
    "    run = client.actor(actor_id).call(run_input=run_input)\n",
    "    dataset_id = run[\"defaultDatasetId\"]\n",
    "    \n",
    "    # Fetch dataset items\n",
    "    dataset_items = []\n",
    "    for item in client.dataset(dataset_id).iterate_items():\n",
    "        dataset_items.append(item)\n",
    "\n",
    "    # Save dataset items to a JSON file\n",
    "    result_file_path = 'tweets.json'\n",
    "    with open(result_file_path, 'w') as file:\n",
    "        json.dump(dataset_items, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Dataset items stored in '{result_file_path}'.\")\n",
    "\n",
    "def filter_dataset_items(input_path, output_path, desired_fields, hashtag_filter):\n",
    "    \"\"\"\n",
    "    Filter dataset items to include only the desired fields and specified hashtag.\n",
    "    \n",
    "    Parameters:\n",
    "    input_path (string): Path to the input JSON file containing the original dataset items.\n",
    "    output_path (string): Path to save the filtered dataset items as a JSON file.\n",
    "    desired_fields (list): List of fields to be included in the filtered dataset items.\n",
    "    hashtag_filter (string): The specific hashtag to filter by (lowercase).\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Load data from the input JSON file\n",
    "    with open(input_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Filter and extract desired fields from data\n",
    "    filtered_data = []\n",
    "    for item in data:\n",
    "        filtered_item = {field: item[field] for field in desired_fields if field in item}\n",
    "        hashtags = item.get('entities', {}).get('hashtags', [])\n",
    "        filtered_item['hashtags'] = [{'text': hashtag['text']} for hashtag in hashtags]\n",
    "        filtered_data.append(filtered_item)\n",
    "\n",
    "    # Save the filtered data to a new JSON file\n",
    "    with open(output_path, 'w') as file:\n",
    "        json.dump(filtered_data, file, indent=4)\n",
    "\n",
    "    print(f'Filtered data has been saved to {output_path}')\n",
    "\n",
    "def data_cleaning():\n",
    "    '''\n",
    "    Clean and transform tweet data for analysis.\n",
    "    \n",
    "    Returns:\n",
    "    top_10 (DataFrame): DataFrame containing the top 10 hashtags by average views count.\n",
    "    extracted_df (DataFrame): DataFrame containing the cleaned tweet data.\n",
    "    '''\n",
    "    # Load the filtered tweet content JSON file into a DataFrame\n",
    "    df = pd.read_json('required_tweet_content.json')\n",
    "\n",
    "    # Check if the DataFrame has the required columns\n",
    "    if 'user_id_str' in df.columns and 'views_count' in df.columns and 'full_text' in df.columns and 'hashtags' in df.columns:\n",
    "        # Initialize an empty list to store the extracted data\n",
    "        extracted_data = []\n",
    "\n",
    "        # Iterate over each row in the DataFrame\n",
    "        for index, row in df.iterrows():\n",
    "            username = row['user_id_str']\n",
    "            views_count = row['views_count']\n",
    "            text = row['full_text']\n",
    "            \n",
    "            # Ensure hashtags are in a list format\n",
    "            if isinstance(row['hashtags'], list):\n",
    "                hashtag_texts = [hashtag['text'] for hashtag in row['hashtags'] if 'text' in hashtag]\n",
    "            else:\n",
    "                hashtag_texts = []  # No hashtags or not in expected format\n",
    "\n",
    "            # Append data to the list\n",
    "            for hashtag_text in hashtag_texts:\n",
    "                extracted_data.append({\n",
    "                    'username': username,\n",
    "                    'views_count': views_count,\n",
    "                    'text': text,\n",
    "                    'hashtag': hashtag_text\n",
    "                })\n",
    "\n",
    "        # Create a DataFrame from the extracted data\n",
    "        extracted_df = pd.DataFrame(extracted_data)\n",
    "\n",
    "    else:\n",
    "        print(\"DataFrame does not have the required columns.\")\n",
    "        return None, None\n",
    "\n",
    "    # Calculate average views count per hashtag\n",
    "    average_likes_per_hashtag = extracted_df.groupby('hashtag')['views_count'].mean().reset_index()\n",
    "\n",
    "    # Get the top 10 hashtags by average views count\n",
    "    top_10 = average_likes_per_hashtag.sort_values(by='views_count', ascending=False).head(20)\n",
    "\n",
    "    return top_10, extracted_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentimental Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define stop words for filtering\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def data_processing(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data.\n",
    "    \n",
    "    Parameters:\n",
    "    text (string): The input text to be processed.\n",
    "    \n",
    "    Returns:\n",
    "    string: The cleaned and processed text.\n",
    "    \"\"\"\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"https\\S+|www\\S+https\\S+\", '', text, flags=re.MULTILINE)\n",
    "    # Remove mentions and hashtags\n",
    "    text = re.sub(r'\\@w+|\\#','', text)\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    # Tokenize text\n",
    "    text_tokens = word_tokenize(text)\n",
    "    # Filter out stop words\n",
    "    filtered_text = [w for w in text_tokens if not w in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "def sentiment(label):\n",
    "    \"\"\"\n",
    "    Determine sentiment based on polarity score.\n",
    "    \n",
    "    Parameters:\n",
    "    label (float): Polarity score.\n",
    "    \n",
    "    Returns:\n",
    "    string: Sentiment category (\"Negative\", \"Neutral\", or \"Positive\").\n",
    "    \"\"\"\n",
    "    if label < 0:\n",
    "        return \"Negative\"\n",
    "    elif label == 0:\n",
    "        return \"Neutral\"\n",
    "    elif label > 0:\n",
    "        return \"Positive\"\n",
    "    \n",
    "def stemming(data):\n",
    "    \"\"\"\n",
    "    Apply stemming to the data.\n",
    "    \n",
    "    Parameters:\n",
    "    data (list): List of words to be stemmed.\n",
    "    \n",
    "    Returns:\n",
    "    list: List of stemmed words.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(word) for word in data]\n",
    "\n",
    "def polarity(text):\n",
    "    \"\"\"\n",
    "    Compute the polarity of the text.\n",
    "    \n",
    "    Parameters:\n",
    "    text (string): The input text.\n",
    "    \n",
    "    Returns:\n",
    "    float: Polarity score.\n",
    "    \"\"\"\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def sentimental_analysis(df):\n",
    "    \"\"\"\n",
    "    Perform sentiment analysis on the text data.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing the text data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with sentiment analysis results.\n",
    "    \"\"\"\n",
    "    # Create a DataFrame for text data\n",
    "    df_text = pd.DataFrame()\n",
    "    df_text['text'] = df['text']\n",
    "    # Apply text processing\n",
    "    df_text['text'] = df_text['text'].apply(data_processing)\n",
    "    # Remove duplicate texts\n",
    "    text_df = df_text.drop_duplicates()\n",
    "    # Apply stemming\n",
    "    text_df['text'] = text_df['text'].apply(lambda x: stemming(x))\n",
    "    # Compute polarity\n",
    "    text_df['polarity'] = text_df['text'].apply(polarity)\n",
    "    # Determine sentiment\n",
    "    text_df['sentiment'] = text_df['polarity'].apply(sentiment)\n",
    "    \n",
    "    return text_df\n",
    "\n",
    "def plots(df, df_2):\n",
    "    \"\"\"\n",
    "    Plot average views count per hashtag and sentiment distribution.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): DataFrame containing hashtag and views count data.\n",
    "    df_2 (DataFrame): DataFrame containing sentiment analysis results.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Plotting Average views count per Hashtag\n",
    "    df = df.dropna(subset=['hashtag'])\n",
    "    plt.bar(df['hashtag'], df['views_count'], color='skyblue')\n",
    "\n",
    "    # Adding title and labels\n",
    "    plt.title('Average views count per Hashtag')\n",
    "    plt.xlabel('Hashtag')\n",
    "    plt.ylabel('Average views count')\n",
    "\n",
    "    # Rotate the x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    hashtag_plot_filename = 'hashtag_plot.png'\n",
    "    plt.savefig(hashtag_plot_filename)\n",
    "    plt.clf()\n",
    "    plt.show()\n",
    "\n",
    "    # Plotting Distribution of sentiments\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    colors = (\"yellowgreen\", \"gold\", \"red\")\n",
    "    wp = {'linewidth': 2, 'edgecolor': \"black\"}\n",
    "    tags = df_2['sentiment'].value_counts()\n",
    "    explode = (0.1, 0.1, 0.1)\n",
    "    tags.plot(kind='pie', autopct='%1.1f%%', shadow=True, colors=colors,\n",
    "              startangle=90, wedgeprops=wp, explode=explode, label='')\n",
    "    plt.title('Distribution of sentiments')\n",
    "    plt.show()\n",
    "    \n",
    "def wordcloud(extract_data):\n",
    "    \"\"\"\n",
    "    Generate a word cloud with color indicating sentiment.\n",
    "    \n",
    "    Parameters:\n",
    "    extract_data (DataFrame): DataFrame containing text and polarity data.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Tokenize the text and calculate the average polarity for each word\n",
    "    word_sentiment = defaultdict(list)\n",
    "    for _, row in extract_data.iterrows():\n",
    "        words = row['text'].split()\n",
    "        for word in words:\n",
    "            word_sentiment[word].append(row['polarity'])\n",
    "\n",
    "    # Calculate average sentiment for each word\n",
    "    word_avg_sentiment = {word: sum(sentiments) / len(sentiments) for word, sentiments in word_sentiment.items()}\n",
    "\n",
    "    # Define a function to determine the color of words in the word cloud\n",
    "    def color_func(word, **kwargs):\n",
    "        sentiment = word_avg_sentiment.get(word, 0)\n",
    "        # Normalize the sentiment score to be between 0 and 1 for the color map\n",
    "        norm_sentiment = (sentiment + 1) / 2\n",
    "        # Get the color from the RdYlGn color map\n",
    "        rgba_color = plt.cm.RdYlGn(norm_sentiment)\n",
    "        # Convert the RGBA color to hex format\n",
    "        return mcolors.rgb2hex(rgba_color)\n",
    "\n",
    "    # Generate word cloud\n",
    "    text = ' '.join(extract_data['text'])\n",
    "    wordcloud = WordCloud(width=800, height=400, color_func=color_func).generate(text)\n",
    "\n",
    "    # Display the generated image\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calling all the functions\n",
    "\n",
    "extract_tweets(api_token,actor_id,searchterm)\n",
    "\n",
    "input_file_path = 'tweets.json'\n",
    "output_file_path = 'required_tweet_content.json'\n",
    "\n",
    "\n",
    "desired_fields = [\n",
    "    \"full_text\", \"lang\", \"reply_count\", \"retweet_count\", \"retweeted\",\n",
    "    \"user_id_str\", \"id_str\", \"url\", \"views_count\", \"created_at\"\n",
    "]\n",
    "\n",
    "filter_dataset_items(input_file_path, output_file_path, desired_fields, 'businessanalysts')\n",
    "\n",
    "\n",
    "\n",
    "top_10, extracted_data = data_cleaning()\n",
    "extract_data = sentimental_analysis(extracted_data)\n",
    "\n",
    "    # Create and show the plots\n",
    "plots(top_10, extract_data)\n",
    "wordcloud(extract_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.image import MIMEImage\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from requests import HTTPError\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/gmail.send\"]\n",
    "\n",
    "# Authentication flow\n",
    "flow = InstalledAppFlow.from_client_secrets_file('credentials.json', SCOPES)\n",
    "creds = flow.run_local_server(port=0)\n",
    "service = build('gmail', 'v1', credentials=creds)\n",
    "\n",
    "# Create a MIMEMultipart message\n",
    "message = MIMEMultipart()\n",
    "message['to'] = 'himanishprakash23@gmail.com, himprakash@ucdavis.edu'\n",
    "message['subject'] = 'Test Mail with Image Attachment'\n",
    "message.attach(MIMEText('Yayy, first attempt successful with image!'))\n",
    "\n",
    "# Attach the image\n",
    "with open(\"sentiment_plot.png\", \"rb\") as image_file:\n",
    "    img_data = image_file.read()\n",
    "image = MIMEImage(img_data)\n",
    "image.add_header('Content-ID', '<image1>')  \n",
    "message.attach(image)\n",
    "\n",
    "# Encode and send the message\n",
    "encoded_message = {'raw': base64.urlsafe_b64encode(message.as_bytes()).decode()}\n",
    "\n",
    "try:\n",
    "    sent_message = service.users().messages().send(userId=\"me\", body=encoded_message).execute()\n",
    "    print(f'Sent message to {message[\"to\"]} Message Id: {sent_message[\"id\"]}')\n",
    "except HTTPError as error:\n",
    "    print(f'An error occurred: {error}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
